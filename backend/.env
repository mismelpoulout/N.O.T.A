# ==============================================
# üîß CONFIGURACI√ìN PRINCIPAL N.O.T.A BACKEND
# ==============================================

# --- Rutas internas ---
DATA_DIR=./backend/data
SQLITE_DB=./backend/data/medical.db
CORPUS_JSONL=./backend/data/corpus.jsonl

# ==============================================
# ü§ñ MODELO LLM (Ollama / LM Studio / OpenAI-Compatible)
# ==============================================

# üëâ Servidor LLM local (por defecto Ollama)
#    Cambia el puerto o la URL si usas otro host o servicio.
LLM_BASE_URL=http://127.0.0.1:11434/v1

# üëâ Modelo activo (elige uno instalado en Ollama)
#    Ejemplo: llama3.2:3b-instruct o llama3.1:8b-instruct
LLM_MODEL=llama3.2:3b-instruct

# üëâ Clave de autenticaci√≥n (no obligatoria en Ollama, pero requerida por compatibilidad)
LLM_API_KEY=ollama

# ==============================================
# ‚öôÔ∏è PAR√ÅMETROS DE SISTEMA
# ==============================================

# Cach√© de respuestas (TTL en segundos)
CACHE_TTL_SECONDS=86400

# ==============================================
# üåç Preferencias de Ranking / B√∫squeda
# ==============================================

# Prioriza dominios confiables en los resultados (separados por coma)
PREFERRED_DOMAINS=minsal.cl,intramed.net,medlineplus.gov,mayoclinic.org

# ==============================================
# üöÄ ENTORNO (opcional, usado en app.py)
# ==============================================
ENV=development
ENABLE_DOCS=true
ALLOWED_ORIGINS=*
TRUSTED_HOSTS=localhost,127.0.0.1
FORCE_HTTPS=false